{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T06:26:08.632544Z",
     "iopub.status.busy": "2025-04-13T06:26:08.632342Z",
     "iopub.status.idle": "2025-04-13T06:26:21.979355Z",
     "shell.execute_reply": "2025-04-13T06:26:21.978502Z",
     "shell.execute_reply.started": "2025-04-13T06:26:08.632526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\n",
      "pandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip3 install -q  wandb\n",
    "!pip install -q pyarrow==18.*\n",
    "!pip install -q ipywidgets==8.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     llm_int8_enable_fp32_cpu_offload=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the synthetic_pii_finance_multilingual dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/gretelai/synthetic_pii_finance_multilingual/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>document_type</th>\n",
       "      <th>document_description</th>\n",
       "      <th>expanded_type</th>\n",
       "      <th>expanded_description</th>\n",
       "      <th>language</th>\n",
       "      <th>language_description</th>\n",
       "      <th>domain</th>\n",
       "      <th>generated_text</th>\n",
       "      <th>pii_spans</th>\n",
       "      <th>conformance_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>bias_score</th>\n",
       "      <th>groundedness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40012</td>\n",
       "      <td>40012</td>\n",
       "      <td>Supply Chain Management Agreement</td>\n",
       "      <td>A legal contract outlining the terms and condi...</td>\n",
       "      <td>Vendor Management Contract</td>\n",
       "      <td>This subtype involves the contractual agreemen...</td>\n",
       "      <td>English</td>\n",
       "      <td>English language as spoken in the United State...</td>\n",
       "      <td>finance</td>\n",
       "      <td>SUPPLY CHAIN MANAGEMENT AGREEMENT\\n\\nThis Supp...</td>\n",
       "      <td>[{\"start\": 119, \"end\": 141, \"label\": \"date\"}, ...</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46425</td>\n",
       "      <td>46425</td>\n",
       "      <td>Supply Chain Management Agreement</td>\n",
       "      <td>A legal contract outlining the terms and condi...</td>\n",
       "      <td>Supply Chain Resilience Framework</td>\n",
       "      <td>This subtype details the framework for buildin...</td>\n",
       "      <td>English</td>\n",
       "      <td>English language as spoken in the United State...</td>\n",
       "      <td>finance</td>\n",
       "      <td>SUPPLY CHAIN RESILIENCE FRAMEWORK\\n\\nThis Supp...</td>\n",
       "      <td>[{\"start\": 119, \"end\": 142, \"label\": \"date\"}, ...</td>\n",
       "      <td>92</td>\n",
       "      <td>87</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4689</td>\n",
       "      <td>4689</td>\n",
       "      <td>Real Estate Loan Agreement</td>\n",
       "      <td>A legal contract outlining terms and condition...</td>\n",
       "      <td>International Real Estate Investment Loan Agre...</td>\n",
       "      <td>This subtype encompasses loans for internation...</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Spanish language as spoken in Spain or Mexico</td>\n",
       "      <td>finance</td>\n",
       "      <td>CONTRATO DE PRÉSTAMO PARA INVERSIÓN INMOBILIAR...</td>\n",
       "      <td>[{\"start\": 182, \"end\": 209, \"label\": \"street_a...</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3002</td>\n",
       "      <td>3002</td>\n",
       "      <td>Real Estate Loan Agreement</td>\n",
       "      <td>A legal contract outlining terms and condition...</td>\n",
       "      <td>Commercial Property Loan Contract</td>\n",
       "      <td>This subtype focuses on loans for commercial r...</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Italian language as spoken in Italy</td>\n",
       "      <td>finance</td>\n",
       "      <td>REPUBBLICA ITALIANA\\n\\nCONTRATTO DI PRESTITO I...</td>\n",
       "      <td>[{\"start\": 85, \"end\": 103, \"label\": \"name\"}, {...</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16187</td>\n",
       "      <td>16187</td>\n",
       "      <td>Email</td>\n",
       "      <td>A communication sent electronically containing...</td>\n",
       "      <td>Invitation</td>\n",
       "      <td>Create an email inviting recipients to an even...</td>\n",
       "      <td>France</td>\n",
       "      <td>French language as spoken in France</td>\n",
       "      <td>finance</td>\n",
       "      <td>Subject: Invitation à notre soirée de lancemen...</td>\n",
       "      <td>[{\"start\": 202, \"end\": 207, \"label\": \"time\"}, ...</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index                      document_type  \\\n",
       "0    40012  40012  Supply Chain Management Agreement   \n",
       "1    46425  46425  Supply Chain Management Agreement   \n",
       "2     4689   4689         Real Estate Loan Agreement   \n",
       "3     3002   3002         Real Estate Loan Agreement   \n",
       "4    16187  16187                              Email   \n",
       "\n",
       "                                document_description  \\\n",
       "0  A legal contract outlining the terms and condi...   \n",
       "1  A legal contract outlining the terms and condi...   \n",
       "2  A legal contract outlining terms and condition...   \n",
       "3  A legal contract outlining terms and condition...   \n",
       "4  A communication sent electronically containing...   \n",
       "\n",
       "                                       expanded_type  \\\n",
       "0                         Vendor Management Contract   \n",
       "1                  Supply Chain Resilience Framework   \n",
       "2  International Real Estate Investment Loan Agre...   \n",
       "3                  Commercial Property Loan Contract   \n",
       "4                                         Invitation   \n",
       "\n",
       "                                expanded_description language  \\\n",
       "0  This subtype involves the contractual agreemen...  English   \n",
       "1  This subtype details the framework for buildin...  English   \n",
       "2  This subtype encompasses loans for internation...  Spanish   \n",
       "3  This subtype focuses on loans for commercial r...  Italian   \n",
       "4  Create an email inviting recipients to an even...   France   \n",
       "\n",
       "                                language_description   domain  \\\n",
       "0  English language as spoken in the United State...  finance   \n",
       "1  English language as spoken in the United State...  finance   \n",
       "2      Spanish language as spoken in Spain or Mexico  finance   \n",
       "3                Italian language as spoken in Italy  finance   \n",
       "4                French language as spoken in France  finance   \n",
       "\n",
       "                                      generated_text  \\\n",
       "0  SUPPLY CHAIN MANAGEMENT AGREEMENT\\n\\nThis Supp...   \n",
       "1  SUPPLY CHAIN RESILIENCE FRAMEWORK\\n\\nThis Supp...   \n",
       "2  CONTRATO DE PRÉSTAMO PARA INVERSIÓN INMOBILIAR...   \n",
       "3  REPUBBLICA ITALIANA\\n\\nCONTRATTO DI PRESTITO I...   \n",
       "4  Subject: Invitation à notre soirée de lancemen...   \n",
       "\n",
       "                                           pii_spans  conformance_score  \\\n",
       "0  [{\"start\": 119, \"end\": 141, \"label\": \"date\"}, ...                 85   \n",
       "1  [{\"start\": 119, \"end\": 142, \"label\": \"date\"}, ...                 92   \n",
       "2  [{\"start\": 182, \"end\": 209, \"label\": \"street_a...                 85   \n",
       "3  [{\"start\": 85, \"end\": 103, \"label\": \"name\"}, {...                 85   \n",
       "4  [{\"start\": 202, \"end\": 207, \"label\": \"time\"}, ...                 85   \n",
       "\n",
       "   quality_score  toxicity_score  bias_score  groundedness_score  \n",
       "0             90               5          15                  95  \n",
       "1             87               5          12                  95  \n",
       "2             90               5          15                  95  \n",
       "3             90               5          10                  95  \n",
       "4             90               5          15                  95  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fields which we will use for finetununf are 'generated_text'  and 'pii_spans'.\n",
    "## We will convert the text into this following format for finetuning\n",
    "```python\n",
    "SUPPLY CHAIN MANAGEMENT AGREEMENT\n",
    "\n",
    "This Supply Chain Management Agreement (the \"Agreement\") is entered into as of this <DATE>1st day of March, 2021</DATE> (the \"Effective Date\"), by and between <COMPANY>Cameron-Mcknight</COMPANY>, a company organized and existing under the laws of the state of Delaware, with its head office located at <STREET_ADDRESS>81685 Lopez Lodge, Apt. 6502</STREET_ADDRESS>, hereinafter referred to as \"<COMPANY>Cameron-Mcknight</COMPANY>\", and <NAME>Jann N. Butte</NAME>, an individual with a mailing address of <STREET_ADDRESS>81685 Lopez Lodge, Apt. 6502</STREET_ADDRESS>.\n",
    "\n",
    "WHEREAS, <COMPANY>Cameron-Mcknight</COMPANY> desires to engage the services of Vendor for the provision of goods and services in connection with <COMPANY>Cameron-Mcknight</COMPANY>'s supply chain operations; and\n",
    "\n",
    "WHEREAS, Vendor desires to provide such goods and services to <COMPANY>Cameron-Mcknight</COMPANY> on the terms and conditions set forth herein.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def add_ner_tags(text, detections):\n",
    "    # Sort detections by start_position in descending order\n",
    "    detections = sorted(detections, key=lambda x: x[\"start\"], reverse=True)\n",
    "    \n",
    "    for entity in detections:\n",
    "        start = entity[\"start\"]\n",
    "        end = entity[\"end\"]\n",
    "        entity_type = entity[\"label\"].upper()\n",
    "        \n",
    "        # Insert end tag first, then start tag\n",
    "        text = text[:end] + f\"</{entity_type}>\" + text[end:]\n",
    "        text = text[:start] + f\"<{entity_type}>\" + text[start:]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tagged_text'] = df.apply(lambda row: add_ner_tags(row['generated_text'], json.loads(row['pii_spans'])), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPPLY CHAIN MANAGEMENT AGREEMENT\n",
      "\n",
      "This Supply Chain Management Agreement (the \"Agreement\") is entered into as of this <DATE>1st day of March, 2021</DATE> (the \"Effective Date\"), by and between <COMPANY>Cameron-Mcknight</COMPANY>, a company organized and existing under the laws of the state of Delaware, with its head office located at <STREET_ADDRESS>81685 Lopez Lodge, Apt. 6502</STREET_ADDRESS>, hereinafter referred to as \"<COMPANY>Cameron-Mcknight</COMPANY>\", and <NAME>Jann N. Butte</NAME>, an individual with a mailing address of <STREET_ADDRESS>81685 Lopez Lodge, Apt. 6502</STREET_ADDRESS>.\n",
      "\n",
      "WHEREAS, <COMPANY>Cameron-Mcknight</COMPANY> desires to engage the services of Vendor for the provision of goods and services in connection with <COMPANY>Cameron-Mcknight</COMPANY>'s supply chain operations; and\n",
      "\n",
      "WHEREAS, Vendor desires to provide such goods and services to <COMPANY>Cameron-Mcknight</COMPANY> on the terms and conditions set forth herein.\n",
      "\n",
      "NOW, THEREFORE, in consideration of the mutual covenants and promises contained herein, the parties hereto agree as follows:\n",
      "\n",
      "1. APPOINTMENT\n",
      "\n",
      "<COMPANY>Cameron-Mcknight</COMPANY> hereby appoints Vendor as a vendor for the provision of goods and services in connection with <COMPANY>Cameron-Mcknight</COMPANY>'s supply chain operations. Vendor shall provide such goods and services in accordance with the terms and conditions set forth herein.\n",
      "\n",
      "2. TERM\n",
      "\n",
      "This Agreement shall commence on the Effective Date and shall continue in force for a period of <DATE>one (1) year</DATE>, unless earlier terminated in accordance with the provisions hereof.\n",
      "\n",
      "3. RESPONSIBILITIES OF VENDOR\n",
      "\n",
      "3.1. Performance. Vendor shall perform its obligations under this Agreement in a professional and workmanlike manner in accordance with industry standards.\n",
      "\n",
      "3.2. Compliance. Vendor shall comply with all applicable laws, rules, and regulations in connection with its performance under this Agreement.\n",
      "\n",
      "3.3. Confidentiality. Vendor shall maintain the confidentiality of all confidential information of <COMPANY>Cameron-Mcknight</COMPANY> and shall not disclose such confidential information to any third party without the prior written consent of <COMPANY>Cameron-Mcknight</COMPANY>.\n",
      "\n",
      "4. PERFORMANCE METRICS\n",
      "\n",
      "Camer\n"
     ]
    }
   ],
   "source": [
    "print(df['tagged_text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will extract the unique entities available in the entire dataset and add a description for each entity \n",
    "## This will be passed to the llm as a Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_entities(df):\n",
    "    unique_entities = set()\n",
    "    \n",
    "    for spans_str in df['pii_spans']:\n",
    "        try:\n",
    "            spans = json.loads(spans_str)\n",
    "            for entity in spans:\n",
    "                unique_entities.add(entity['label'].upper())\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            continue\n",
    "    return unique_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_entities = get_unique_entities(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_description_dict = {\n",
    "    \"ACCOUNT_PIN\": \"Personal Identification Number for accounts\",\n",
    "    \"API_KEY\": \"Authentication token for accessing APIs\",\n",
    "    \"BANK_ROUTING_NUMBER\": \"Bank’s routing number used in financial transactions\",\n",
    "    \"BBAN\": \"Basic Bank Account Number used in international banking\",\n",
    "    \"COMPANY\": \"Name of a business or organization\",\n",
    "    \"CREDIT_CARD_NUMBER\": \"Number on a credit card used for payments\",\n",
    "    \"CREDIT_CARD_SECURITY_CODE\": \"CVV or security code on a credit card\",\n",
    "    \"CUSTOMER_ID\": \"Unique identifier assigned to a customer\",\n",
    "    \"DATE\": \"Calendar date (e.g. 2025-04-10)\",\n",
    "    \"DATE_OF_BIRTH\": \"Person’s date of birth\",\n",
    "    \"DRIVER_LICENSE_NUMBER\": \"License number assigned to a driver\",\n",
    "    \"EMAIL\": \"Email address\",\n",
    "    \"EMPLOYEE_ID\": \"Identifier assigned to an employee\",\n",
    "    \"FIRST_NAME\": \"Given name of a person\",\n",
    "    \"IBAN\": \"International Bank Account Number used in global banking\",\n",
    "    \"IPV4\": \"IPv4 format IP address (e.g. 192.168.1.1)\",\n",
    "    \"IPV6\": \"IPv6 format IP address\",\n",
    "    \"LAST_NAME\": \"Surname or family name of a person\",\n",
    "    \"LOCAL_LATLNG\": \"Geolocation data in latitude and longitude format\",\n",
    "    \"NAME\": \"Full name of a person\",\n",
    "    \"PASSPORT_NUMBER\": \"Number identifying a person’s passport\",\n",
    "    \"PASSWORD\": \"Confidential login credential\",\n",
    "    \"SSN\": \"Social Security Number (often US)\",\n",
    "    \"STREET_ADDRESS\": \"Street-level residential or business address\",\n",
    "    \"SWIFT_BIC_CODE\": \"Bank Identifier Code used for international wires\",\n",
    "    \"TIME\": \"Time of day (e.g. 14:00, 2 PM)\",\n",
    "    \"USER_NAME\": \"Username used for login or identification\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Unsloth llama model for fine tuning . Unsloth is Opensource Library for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:09:17.490671Z",
     "iopub.status.busy": "2025-04-12T06:09:17.490402Z",
     "iopub.status.idle": "2025-04-12T06:10:00.669930Z",
     "shell.execute_reply": "2025-04-12T06:10:00.669105Z",
     "shell.execute_reply.started": "2025-04-12T06:09:17.490651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 06:09:27.899974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744438168.132798      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744438168.198543      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53922ff336d4889b024205e56e04c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78556da7eca54f31a2bfd91189745c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# import bitsandbytes\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\",token=secret_value_0)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\",token=secret_value_0,\n",
    "#                                              device_map=\"auto\",\n",
    "#                                              quantization_config = bnb_config)\n",
    "# processor = AutoProcessor.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
    "    max_seq_length=8192,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    "    gpu_memory_utilization=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.2\",  # or \"chatml\", \"alpaca\", etc. based on how you finetuned\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    "    map_eos_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:13:44.385157Z",
     "iopub.status.busy": "2025-04-12T06:13:44.384357Z",
     "iopub.status.idle": "2025-04-12T06:13:44.389171Z",
     "shell.execute_reply": "2025-04-12T06:13:44.388324Z",
     "shell.execute_reply.started": "2025-04-12T06:13:44.385123Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "torch.cuda.empty_cache() \n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"max_split_size_mb:100\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I am goining to use instruction based fine tuning. For that I am using Chain-Of-Thought in the prompt design for instructing the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First user message\n",
    "usr_msg1 = f\"\"\"You are given a user utterance that may contain Personal Identifiable Information (PII). \n",
    "        You are also given a list of entity types representing Personal Identifiable Information (PII). \n",
    "        Your task is to detect and identify all instances of the supplied PII entity types in the user utterance. \n",
    "        The output must have the same content as the input. Only the tokens that match the PII entities in the \n",
    "        list should be enclosed within XML tags. The XML tag comes from the PII entities described in the list below. \n",
    "        For example, a given name of a person should be enclosed within <FIRST_NAME></FIRST_NAME> tags.\n",
    "        Ensure that all entities are identified. Do not perform false identifications.\n",
    "        \\n\\nList Of Entities\\n{json.dumps(description_dict,indent=2)}\n",
    "        \\n\\n\n",
    "        Are the instructions clear to you?\"\"\"\n",
    "\n",
    "# First assistant message which is a reply to usr_msg1\n",
    "# I hardcode this msg once the model produced a resonably good response.\n",
    "# Note that the response comes from a non-fine-tuned version of the model.\n",
    "# The idea is to sample a good 'thought' from the base version of the model.    \n",
    "asst_msg1 = \"\"\"Yes, the instructions are clear. I will identify and enclose within the corresponding XML tags, \n",
    "    all instances of the specified PII entity types in the user utterance. For example, \n",
    "    <FIRST_NAME><Given name of a person></FIRST_NAME>, <PASSPORT_NUMBER><Number identifying a person’s passport></PASSPORT_NUMBER>, etc. \n",
    "    leaving the rest of the user utterance unchanged.\"\"\"\n",
    "\n",
    "# Here I hardcode a few shot example as a user message\n",
    "usr_msg2 = \"My name is John Doe, and I can be contacted at meet_me_john@gmail.com\"\n",
    "\n",
    "# Here I hardcode the appropriate response from the assitant as the\n",
    "# correct output of the few shot example\n",
    "asst_msg2 = \"My name is <FIRST_NAME>John</FIRST_NAME> <LAST_NAME>Doe</LAST_NAME>, and I can be contacted at <EMAIL>meet_me_john@gmail.com</EMAIL>\"\n",
    "\n",
    "# Here I ask the assistant why its response to the last user message was the \n",
    "# correct response\n",
    "usr_msg3 = \"Give a brief explanation of why your answer is correct.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\"role\": \"user\", \"content\": usr_msg1},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg1},\n",
    "        {\"role\": \"user\", \"content\": usr_msg2},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg2},\n",
    "        {\"role\": \"user\", \"content\": usr_msg3},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True,return_tensors=\"pt\",add_generation_prompt=True)\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=512) \n",
    "output_text = tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the response I got from the assistant . \n",
    "# I have Hardcoded this into the prompt generating function\n",
    "asst_msg3 = re.findall(r'<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>', asst_msg3, re.DOTALL)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My answer is correct because I identified the specified PII entity types in the user utterance and enclosed them within the corresponding XML tags.\n",
      "\n",
      "                    - \"John\" is a given name, so it was enclosed within <FIRST_NAME> tags.\n",
      "                    - \"Doe\" is a surname, so it was enclosed within <LAST_NAME> tags.\n",
      "                    - \"meet_me_john@gmail.com\" is an email address, so it was enclosed within <EMAIL> tags.\n"
     ]
    }
   ],
   "source": [
    "print(asst_msg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The function which creates the entire training data. \n",
    "## The data looks like this\n",
    "```text\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 10 Apr 2025\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "You are given a user utterance that may contain Personal Identifiable Information (PII). \n",
    "        You are also given a list of entity types representing Personal Identifiable Information (PII). \n",
    "        Your task is to detect and identify all instances of the supplied PII entity types in the user utterance. \n",
    "        The output must have the same content as the input. Only the tokens that match the PII entities in the \n",
    "        list should be enclosed within XML tags. The XML tag comes from the PII entities described in the list below. \n",
    "        For example, a given name of a person should be enclosed within <FIRST_NAME></FIRST_NAME> tags.\n",
    "        Ensure that all entities are identified. Do not perform false identifications.\n",
    "        \n",
    "\n",
    "List Of Entities\n",
    "{\n",
    "  \"ACCOUNT_PIN\": \"Personal Identification Number for accounts\",\n",
    "  \"API_KEY\": \"Authentication token for accessing APIs\",\n",
    "  \"BANK_ROUTING_NUMBER\": \"Bank\\u2019s routing number used in financial transactions\",\n",
    "  \"BBAN\": \"Basic Bank Account Number used in international banking\",\n",
    "  \"COMPANY\": \"Name of a business or organization\",\n",
    "  \"CREDIT_CARD_NUMBER\": \"Number on a credit card used for payments\",\n",
    "  \"CREDIT_CARD_SECURITY_CODE\": \"CVV or security code on a credit card\",\n",
    "  \"CUSTOMER_ID\": \"Unique identifier assigned to a customer\",\n",
    "  \"DATE\": \"Calendar date (e.g. 2025-04-10)\",\n",
    "  \"DATE_OF_BIRTH\": \"Person\\u2019s date of birth\",\n",
    "  \"DRIVER_LICENSE_NUMBER\": \"License number assigned to a driver\",\n",
    "  \"EMAIL\": \"Email address\",\n",
    "  \"EMPLOYEE_ID\": \"Identifier assigned to an employee\",\n",
    "  \"FIRST_NAME\": \"Given name of a person\",\n",
    "  \"IBAN\": \"International Bank Account Number used in global banking\",\n",
    "  \"IPV4\": \"IPv4 format IP address (e.g. 192.168.1.1)\",\n",
    "  \"IPV6\": \"IPv6 format IP address\",\n",
    "  \"LAST_NAME\": \"Surname or family name of a person\",\n",
    "  \"LOCAL_LATLNG\": \"Geolocation data in latitude and longitude format\",\n",
    "  \"NAME\": \"Full name of a person\",\n",
    "  \"PASSPORT_NUMBER\": \"Number identifying a person\\u2019s passport\",\n",
    "  \"PASSWORD\": \"Confidential login credential\",\n",
    "  \"SSN\": \"Social Security Number (often US)\",\n",
    "  \"STREET_ADDRESS\": \"Street-level residential or business address\",\n",
    "  \"SWIFT_BIC_CODE\": \"Bank Identifier Code used for international wires\",\n",
    "  \"TIME\": \"Time of day (e.g. 14:00, 2 PM)\",\n",
    "  \"USER_NAME\": \"Username used for login or identification\"\n",
    "}\n",
    "        \n",
    "\n",
    "\n",
    "        Are the instructions clear to you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "Yes, the instructions are clear. I will identify and enclose within the corresponding XML tags, \n",
    "        all instances of the specified PII entity types in the user utterance. For example, \n",
    "        <FIRST_NAME><Given name of a person></FIRST_NAME>, <PASSPORT_NUMBER><Number identifying a person’s passport></PASSPORT_NUMBER>, etc. \n",
    "        leaving the rest of the user utterance unchanged.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "My name is John Doe, and I can be contacted at meet_me_john@gmail.com<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "My name is <FIRST_NAME>John</FIRST_NAME> <LAST_NAME>Doe</LAST_NAME>, and I can be contacted at <EMAIL>meet_me_john@gmail.com</EMAIL><|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Give a brief explanation of why your answer is correct.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "My answer is correct because I identified the specified PII entity types in the user utterance and enclosed them within the corresponding XML tags.\n",
    "\n",
    "                    - \"John\" is a given name, so it was enclosed within <FIRST_NAME> tags.\n",
    "                    - \"Doe\" is a surname, so it was enclosed within <LAST_NAME> tags.\n",
    "                    - \"meet_me_john@gmail.com\" is an email address, so it was enclosed within <EMAIL> tags.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Great! I am now going to give you another user utterance. Please detect PII entities in it according to the previous instructions. Do not include an explanation in your answer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "Sure! Please give me the user utterance.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "SUPPLY CHAIN MANAGEMENT AGREEMENT\n",
    "\n",
    "This Supply Chain Management Agreement (the \"Agreement\") is entered into as of this 1st day of March, 2021 (the \"Effective Date\"), by and between Cameron-Mcknight, a company organized and existing under the laws of the state of Delaware, with its head office located at 81685 Lopez Lodge, Apt. 6502, hereinafter referred to as \"Cameron-Mcknight\", and Jann N. Butte, an individual with a mailing address of 81685 Lopez Lodge, Apt. 6502.\n",
    "\n",
    "WHEREAS, Cameron-Mcknight desires to engage the services of Vendor for the provision of goods and services in connection with Cameron-Mcknight's supply chain operations; and\n",
    "\n",
    "WHEREAS, Vendor desires to provide such goods and services to Cameron-Mcknight on the terms and conditions set forth herein.\n",
    "\n",
    "NOW, THEREFORE, in consideration of the mutual covenants and promises contained herein, the parties hereto agree as follows:\n",
    "\n",
    "1. APPOINTMENT\n",
    "\n",
    "Cameron-Mcknight hereby appoints Vendor as a vendor for the provision of goods and services in connection with Cameron-Mcknight's supply chain operations. Vendor shall provide such goods and services in accordance with the terms and conditions set forth herein.\n",
    "\n",
    "2. TERM\n",
    "\n",
    "This Agreement shall commence on the Effective Date and shall continue in force for a period of one (1) year, unless earlier terminated in accordance with the provisions hereof.\n",
    "\n",
    "3. RESPONSIBILITIES OF VENDOR\n",
    "\n",
    "3.1. Performance. Vendor shall perform its obligations under this Agreement in a professional and workmanlike manner in accordance with industry standards.\n",
    "\n",
    "3.2. Compliance. Vendor shall comply with all applicable laws, rules, and regulations in connection with its performance under this Agreement.\n",
    "\n",
    "3.3. Confidentiality. Vendor shall maintain the confidentiality of all confidential information of Cameron-Mcknight and shall not disclose such confidential information to any third party without the prior written consent of Cameron-Mcknight.\n",
    "\n",
    "4. PERFORMANCE METRICS\n",
    "\n",
    "Camer<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "SUPPLY CHAIN MANAGEMENT AGREEMENT\n",
    "\n",
    "This Supply Chain Management Agreement (the \"Agreement\") is entered into as of this <DATE>1st day of March, 2021</DATE> (the \"Effective Date\"), by and between <COMPANY>Cameron-Mcknight</COMPANY>, a company organized and existing under the laws of the state of Delaware, with its head office located at <STREET_ADDRESS>81685 Lopez Lodge, Apt. 6502</STREET_ADDRESS>, hereinafter referred to as \"<COMPANY>Cameron-Mcknight</COMPANY>\", and <NAME>Jann N. Butte</NAME>, an individual with a mailing address of <STREET_ADDRESS>81685 Lopez Lodge, Apt. 6502</STREET_ADDRESS>.\n",
    "\n",
    "WHEREAS, <COMPANY>Cameron-Mcknight</COMPANY> desires to engage the services of Vendor for the provision of goods and services in connection with <COMPANY>Cameron-Mcknight</COMPANY>'s supply chain operations; and\n",
    "\n",
    "WHEREAS, Vendor desires to provide such goods and services to <COMPANY>Cameron-Mcknight</COMPANY> on the terms and conditions set forth herein.\n",
    "\n",
    "NOW, THEREFORE, in consideration of the mutual covenants and promises contained herein, the parties hereto agree as follows:\n",
    "\n",
    "1. APPOINTMENT\n",
    "\n",
    "<COMPANY>Cameron-Mcknight</COMPANY> hereby appoints Vendor as a vendor for the provision of goods and services in connection with <COMPANY>Cameron-Mcknight</COMPANY>'s supply chain operations. Vendor shall provide such goods and services in accordance with the terms and conditions set forth herein.\n",
    "\n",
    "2. TERM\n",
    "\n",
    "This Agreement shall commence on the Effective Date and shall continue in force for a period of <DATE>one (1) year</DATE>, unless earlier terminated in accordance with the provisions hereof.\n",
    "\n",
    "3. RESPONSIBILITIES OF VENDOR\n",
    "\n",
    "3.1. Performance. Vendor shall perform its obligations under this Agreement in a professional and workmanlike manner in accordance with industry standards.\n",
    "\n",
    "3.2. Compliance. Vendor shall comply with all applicable laws, rules, and regulations in connection with its performance under this Agreement.\n",
    "\n",
    "3.3. Confidentiality. Vendor shall maintain the confidentiality of all confidential information of <COMPANY>Cameron-Mcknight</COMPANY> and shall not disclose such confidential information to any third party without the prior written consent of <COMPANY>Cameron-Mcknight</COMPANY>.\n",
    "\n",
    "4. PERFORMANCE METRICS\n",
    "\n",
    "Camer<|eot_id|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  transformers import PreTrainedTokenizerBase\n",
    "def get_fine_tune_prompt_xml(\n",
    "    description_dict: dict, \n",
    "    input_str: str,\n",
    "    label_str: str,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    description (dict): dict of strings representing entity labels and its\n",
    "                          corresponding description\n",
    "    input_str (str): Actual input string on which detections need to be\n",
    "                     performed\n",
    "    label_str (str): Expected output string corresponding to input_str\n",
    "    tokenizer (PreTrainedTokenizerBase): A tokenizer corresponding to the model\n",
    "                                         being fine-tuned\n",
    "\n",
    "    Returns: \n",
    "    torch.Tensor: Tensor of tokenized input ids\n",
    "    \"\"\"\n",
    "\n",
    "    usr_msg1 = f\"\"\"You are given a user utterance that may contain Personal Identifiable Information (PII). \n",
    "        You are also given a list of entity types representing Personal Identifiable Information (PII). \n",
    "        Your task is to detect and identify all instances of the supplied PII entity types in the user utterance. \n",
    "        The output must have the same content as the input. Only the tokens that match the PII entities in the \n",
    "        list should be enclosed within XML tags. The XML tag comes from the PII entities described in the list below. \n",
    "        For example, a given name of a person should be enclosed within <FIRST_NAME></FIRST_NAME> tags.\n",
    "        Ensure that all entities are identified. Do not perform false identifications.\n",
    "        \\n\\nList Of Entities\\n{json.dumps(description_dict,indent=2)}\n",
    "        \\n\\n\n",
    "        Are the instructions clear to you?\"\"\"\n",
    "    \n",
    "    asst_msg1 = \"\"\"Yes, the instructions are clear. I will identify and enclose within the corresponding XML tags, \n",
    "        all instances of the specified PII entity types in the user utterance. For example, \n",
    "        <FIRST_NAME><Given name of a person></FIRST_NAME>, <PASSPORT_NUMBER><Number identifying a person’s passport></PASSPORT_NUMBER>, etc. \n",
    "        leaving the rest of the user utterance unchanged.\"\"\"\n",
    "    \n",
    "    usr_msg2 = \"My name is John Doe, and I can be contacted at meet_me_john@gmail.com\"\n",
    "    \n",
    "    asst_msg2 = \"My name is <FIRST_NAME>John</FIRST_NAME> <LAST_NAME>Doe</LAST_NAME>, and I can be contacted at <EMAIL>meet_me_john@gmail.com</EMAIL>\"\n",
    "    \n",
    "    usr_msg3 = \"Give a brief explanation of why your answer is correct.\"\n",
    "\n",
    "\n",
    "    asst_msg3 = \"\"\"My answer is correct because I identified the specified PII entity types in the user utterance and enclosed them within the corresponding XML tags.\n",
    "\n",
    "                    - \"John\" is a given name, so it was enclosed within <FIRST_NAME> tags.\n",
    "                    - \"Doe\" is a surname, so it was enclosed within <LAST_NAME> tags.\n",
    "                    - \"meet_me_john@gmail.com\" is an email address, so it was enclosed within <EMAIL> tags.\"\"\"\n",
    "    \n",
    "    usr_msg4 = \"\"\"Great! I am now going to give you another user utterance. Please detect PII entities in it according to the previous instructions. Do not include an explanation in your answer.\"\"\"\n",
    "    \n",
    "    asst_msg4 = \"Sure! Please give me the user utterance.\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": usr_msg1},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg1},\n",
    "        {\"role\": \"user\", \"content\": usr_msg2},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg2},\n",
    "        {\"role\": \"user\", \"content\": usr_msg3},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg3},\n",
    "        {\"role\": \"user\", \"content\": usr_msg4},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg4},\n",
    "        {\"role\": \"user\", \"content\": input_str},\n",
    "        {\"role\": \"assistant\", \"content\": label_str},\n",
    "    ]\n",
    "    \n",
    "    encoded_input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "    return tokenizer.decode(encoded_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df.apply(lambda row: get_fine_tune_prompt_xml(entity_description_dict,row['generated_text'], row['tagged_text'],tokenizer), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Masking\n",
    "- This will help the model to calculate the loss only on the generated part . not on the entire given input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:14:05.551511Z",
     "iopub.status.busy": "2025-04-12T06:14:05.551261Z",
     "iopub.status.idle": "2025-04-12T06:14:05.562627Z",
     "shell.execute_reply": "2025-04-12T06:14:05.561717Z",
     "shell.execute_reply.started": "2025-04-12T06:14:05.551491Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.utils import PaddingStrategy\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "@dataclass\n",
    "class CustomDataCollatorWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "\n",
    "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence is provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        return_tensors (`str`, *optional*, defaults to `\"pt\"`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        \n",
    "        # Set loss mask for all pad tokens\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        # Compute loss mask for appropriate tokens only\n",
    "        for i in range(batch['input_ids'].shape[0]):\n",
    "            \n",
    "            # Decode the training input\n",
    "            text_content = self.tokenizer.decode(batch['input_ids'][i][1:])  # slicing from [1:] is important because tokenizer adds bos token\n",
    "            \n",
    "            # Extract substrings for prompt text in the training input\n",
    "            # The training input ends at the last user msg ending in [/INST]\n",
    "            prompt_gen_boundary = text_content.rfind(\"<|end_header_id|>\") + len(\"<|end_header_id|>\")\n",
    "            prompt_text = text_content[:prompt_gen_boundary]\n",
    "            \n",
    "            # print(f\"\"\"PROMPT TEXT:\\n{prompt_text}\"\"\")\n",
    "            \n",
    "            # retokenize the prompt text only\n",
    "            prompt_text_tokenized = self.tokenizer(\n",
    "                prompt_text,\n",
    "                return_overflowing_tokens=False,\n",
    "                return_length=False,\n",
    "            )\n",
    "            # compute index where prompt text ends in the training input\n",
    "            prompt_tok_idx = len(prompt_text_tokenized['input_ids'])\n",
    "            \n",
    "            # Set loss mask for all tokens in prompt text\n",
    "            labels[i][range(prompt_tok_idx)] = -100\n",
    "            \n",
    "            # print(\"================DEBUGGING INFORMATION===============\")\n",
    "            # for idx, tok in enumerate(labels[i]):\n",
    "            #     token_id = batch['input_ids'][i][idx]\n",
    "            #     decoded_token_id = self.tokenizer.decode(batch['input_ids'][i][idx])\n",
    "            #     print(f\"\"\"TOKID: {token_id} | LABEL: {tok} || DECODED: {decoded_token_id}\"\"\")\n",
    "                    \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:14:05.563814Z",
     "iopub.status.busy": "2025-04-12T06:14:05.563539Z",
     "iopub.status.idle": "2025-04-12T06:14:05.577590Z",
     "shell.execute_reply": "2025-04-12T06:14:05.576932Z",
     "shell.execute_reply.started": "2025-04-12T06:14:05.563790Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"Instruct: <prompt>\\nOutput:\"\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset object and splitting them into train, test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:14:05.591772Z",
     "iopub.status.busy": "2025-04-12T06:14:05.591592Z",
     "iopub.status.idle": "2025-04-12T06:14:10.045284Z",
     "shell.execute_reply": "2025-04-12T06:14:10.044723Z",
     "shell.execute_reply.started": "2025-04-12T06:14:05.591758Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df[['text']]\n",
    "# df = df.rename(columns={'generated_text':'text'})\n",
    "dataset = ds.dataset(pa.Table.from_pandas(df))\n",
    "# ### convert to Huggingface dataset\n",
    "hg_dataset = Dataset(pa.Table.from_pandas(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:14:10.046244Z",
     "iopub.status.busy": "2025-04-12T06:14:10.046027Z",
     "iopub.status.idle": "2025-04-12T06:14:10.076228Z",
     "shell.execute_reply": "2025-04-12T06:14:10.075535Z",
     "shell.execute_reply.started": "2025-04-12T06:14:10.046228Z"
    }
   },
   "outputs": [],
   "source": [
    "train_testvalid = hg_dataset.train_test_split(test_size=0.2)\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:14:10.077312Z",
     "iopub.status.busy": "2025-04-12T06:14:10.077004Z",
     "iopub.status.idle": "2025-04-12T06:14:10.082093Z",
     "shell.execute_reply": "2025-04-12T06:14:10.081351Z",
     "shell.execute_reply.started": "2025-04-12T06:14:10.077292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 40276\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5035\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5035\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I am using the Unsloth Where I have mentioned the finetuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:14:10.083778Z",
     "iopub.status.busy": "2025-04-12T06:14:10.083586Z",
     "iopub.status.idle": "2025-04-12T06:14:15.868332Z",
     "shell.execute_reply": "2025-04-12T06:14:15.867506Z",
     "shell.execute_reply.started": "2025-04-12T06:14:10.083757Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 5636096 | total: 1241450496 | Percentage: 0.4540%\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state = 32,\n",
    "    loftq_config = None,\n",
    ")\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the SFTTrainer\n",
    "\n",
    "## Trained the model on two epochs which took 30+ hours to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:14:19.504881Z",
     "iopub.status.busy": "2025-04-12T06:14:19.504256Z",
     "iopub.status.idle": "2025-04-12T06:16:35.362277Z",
     "shell.execute_reply": "2025-04-12T06:16:35.361383Z",
     "shell.execute_reply.started": "2025-04-12T06:14:19.504856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13afe12a89184be58f8aced8a830c47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bfcd84b9b87450d82d58e985ea1f569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029d6162564c4f8b9179b79a3f562195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa43bc222964611b2b4c8de89f25b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/40276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2678904333204254a81a76c883c416ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/5035 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from trl import SFTTrainer,SFTConfig\n",
    "max_seq_length = 8192\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc = 2,\n",
    "        learning_rate=3e-4,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        per_device_train_batch_size=16,\n",
    "        gradient_accumulation_steps=4,\n",
    "        gradient_checkpointing=True,\n",
    "        num_train_epochs=2,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=10,\n",
    "        save_strategy=\"steps\", \n",
    "        save_steps=50, \n",
    "        output_dir=\"/kaggle/working/Llama-3.2-1B-Instruct-SFT/output\",\n",
    "        seed=0,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_test_valid_dataset[\"train\"],\n",
    "    eval_dataset=train_test_valid_dataset[\"valid\"],\n",
    "    # peft_config=lora_config,\n",
    "    args=training_arguments,\n",
    "    # Using custom data collator inside SFTTrainer\n",
    "    data_collator=CustomDataCollatorWithPadding(\n",
    "        tokenizer=tokenizer, \n",
    "        padding=\"longest\", \n",
    "        max_length=max_seq_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using wandb for getting the training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:16:35.364093Z",
     "iopub.status.busy": "2025-04-12T06:16:35.363804Z",
     "iopub.status.idle": "2025-04-12T06:16:37.131748Z",
     "shell.execute_reply": "2025-04-12T06:16:37.130928Z",
     "shell.execute_reply.started": "2025-04-12T06:16:35.364067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login --relogin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I have trained the model in multiple chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:16:37.133212Z",
     "iopub.status.busy": "2025-04-12T06:16:37.132930Z",
     "iopub.status.idle": "2025-04-12T10:49:04.506482Z",
     "shell.execute_reply": "2025-04-12T10:49:04.505809Z",
     "shell.execute_reply.started": "2025-04-12T06:16:37.133188Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 40,276 | Num Epochs = 2 | Total steps = 628\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 4 x 1) = 128\n",
      " \"-____-\"     Trainable parameters = 5,636,096/1,000,000,000 (0.56% trained)\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tsave_steps: 50 (from args) != 500 (from trainer_state.json)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miamarpanghosh\u001b[0m (\u001b[33marpx\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250412_061645-saje4zlt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arpx/huggingface/runs/saje4zlt' target=\"_blank\">/kaggle/working/Llama-3.2-1B-Instruct-SFT/output</a></strong> to <a href='https://wandb.ai/arpx/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arpx/huggingface' target=\"_blank\">https://wandb.ai/arpx/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arpx/huggingface/runs/saje4zlt' target=\"_blank\">https://wandb.ai/arpx/huggingface/runs/saje4zlt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='628' max='628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [628/628 4:29:57, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>0.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>0.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>0.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>517</td>\n",
       "      <td>0.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>0.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>519</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>521</td>\n",
       "      <td>0.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>522</td>\n",
       "      <td>0.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>0.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>0.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>0.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>0.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>529</td>\n",
       "      <td>0.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>0.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>0.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>0.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>539</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>547</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>549</td>\n",
       "      <td>0.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>0.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>0.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>559</td>\n",
       "      <td>0.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561</td>\n",
       "      <td>0.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>562</td>\n",
       "      <td>0.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>0.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>0.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>569</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>571</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>0.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>573</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>0.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>577</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>578</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>579</td>\n",
       "      <td>0.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>0.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>0.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>589</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>591</td>\n",
       "      <td>0.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>0.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>593</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>596</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597</td>\n",
       "      <td>0.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>604</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>606</td>\n",
       "      <td>0.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>611</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>0.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>0.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>617</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>618</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>0.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>0.025400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>627</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>628</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=628, training_loss=0.0055916392226602624, metrics={'train_runtime': 16344.6697, 'train_samples_per_second': 4.928, 'train_steps_per_second': 0.038, 'total_flos': 9.014416881376297e+17, 'train_loss': 0.0055916392226602624})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T11:17:13.640012Z",
     "iopub.status.busy": "2025-04-12T11:17:13.639417Z",
     "iopub.status.idle": "2025-04-12T11:17:13.993600Z",
     "shell.execute_reply": "2025-04-12T11:17:13.992791Z",
     "shell.execute_reply.started": "2025-04-12T11:17:13.639989Z"
    }
   },
   "source": [
    "## Model is available here https://huggingface.co/Arpx22/llama-3.2-1B-Finetuned-ner-finance/tree/main/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T10:52:49.089151Z",
     "iopub.status.busy": "2025-04-12T10:52:49.088461Z",
     "iopub.status.idle": "2025-04-12T10:52:53.520195Z",
     "shell.execute_reply": "2025-04-12T10:52:53.519613Z",
     "shell.execute_reply.started": "2025-04-12T10:52:49.089111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de7aecf90d446dc8a14ff9e293be09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/22.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49ff5e5cfb7495bb528fb9087b75b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 9 LFS files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c5c7cf91c3405dae83fcf248a48e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1744348716.39b6fb311824.390.0:   0%|          | 0.00/72.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65454d9f7f3c40e09d9ff619e340cd0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1744392197.e36a035d718e.119.0:   0%|          | 0.00/6.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee89f03aeea941bb8b78484987b62fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1744348130.39b6fb311824.31.1:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1d5cae338f4c0d9b247462b459f0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1744347954.39b6fb311824.31.0:   0%|          | 0.00/6.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c93cb689be54d1f975ce432b20a9b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1744392434.e36a035d718e.119.1:   0%|          | 0.00/65.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e974b14b8d466bb70a3f9a48faea27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1744438599.c1d106e731ad.31.0:   0%|          | 0.00/33.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4784a0a163864225bcd9f38373a99c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcfc751829e44139e478629a216e86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Arpx22/output/commit/f7e33713d8ee9354eb502743b1dbbcc1fbddb3b5', commit_message='End of training', commit_description='', oid='f7e33713d8ee9354eb502743b1dbbcc1fbddb3b5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Arpx22/output', endpoint='https://huggingface.co', repo_type='model', repo_id='Arpx22/output'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T10:51:04.729554Z",
     "iopub.status.busy": "2025-04-12T10:51:04.729252Z",
     "iopub.status.idle": "2025-04-12T10:51:04.742574Z",
     "shell.execute_reply": "2025-04-12T10:51:04.741820Z",
     "shell.execute_reply.started": "2025-04-12T10:51:04.729534Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T10:51:21.337409Z",
     "iopub.status.busy": "2025-04-12T10:51:21.337110Z",
     "iopub.status.idle": "2025-04-12T10:51:21.354787Z",
     "shell.execute_reply": "2025-04-12T10:51:21.354001Z",
     "shell.execute_reply.started": "2025-04-12T10:51:21.337389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a1525158b64c2da489bd2d637028dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T10:59:52.206208Z",
     "iopub.status.busy": "2025-04-12T10:59:52.205570Z",
     "iopub.status.idle": "2025-04-12T10:59:58.315565Z",
     "shell.execute_reply": "2025-04-12T10:59:58.314813Z",
     "shell.execute_reply.started": "2025-04-12T10:59:52.206180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef59890ab5047ef9f7596664382e4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d2e1f3b2e14d03b32906a16a78065d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/22.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/Arpx22/llama-3.2-1B-Finetuned-ner-finance\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e922fc568f964de781ce5f2685bcc20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.push_to_hub(repo_id = \"Arpx22/llama-3.2-1B-Finetuned-ner-finance\")\n",
    "tokenizer.push_to_hub(repo_id = \"Arpx22/llama-3.2-1B-Finetuned-ner-finance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is an example of testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"Indian Overseas Bank reduces Repo Linked Lending Rate by 25 basis points\n",
    "Indian Overseas Bank has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
    "\n",
    "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
    "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
    "\n",
    "India's auto component industry to hit USD 145 billion by 2030, exports to triple: NITI Aayog\n",
    "NITI Aayog envisions India's automotive component production soaring to USD 145 billion by 2030, with exports tripling to USD 60 billion. A ...\n",
    "\n",
    "Banks scouting buyers for Sahara Star Hotel loans\n",
    "Lenders led by Union Bank of India are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
    "\n",
    "Global media titans to converge in Mumbai for first-ever WAVES summit in May\n",
    "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from May 1-4, backed by the Indian government. ...\n",
    "\n",
    "India is the place to be: Radisson Top Exec\n",
    "Radisson Hotel Group is bullish on India's hotel development potential, aiming to reach 500 hotels by the end of the decade by signing 50 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  transformers import PreTrainedTokenizerBase\n",
    "import json\n",
    "def testing_prompt_xml(\n",
    "    description_dict: dict, \n",
    "    input_str: str,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    ") :\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    description (dict): dict of strings representing entity labels and its\n",
    "                          corresponding description\n",
    "    input_str (str): Actual input string on which detections need to be\n",
    "                     performed\n",
    "    label_str (str): Expected output string corresponding to input_str\n",
    "    tokenizer (PreTrainedTokenizerBase): A tokenizer corresponding to the model\n",
    "                                         being fine-tuned\n",
    "\n",
    "    Returns: \n",
    "    torch.Tensor: Tensor of tokenized input ids\n",
    "    \"\"\"\n",
    "\n",
    "    usr_msg1 = f\"\"\"You are given a user utterance that may contain Personal Identifiable Information (PII). \n",
    "        You are also given a list of entity types representing Personal Identifiable Information (PII). \n",
    "        Your task is to detect and identify all instances of the supplied PII entity types in the user utterance. \n",
    "        The output must have the same content as the input. Only the tokens that match the PII entities in the \n",
    "        list should be enclosed within XML tags. The XML tag comes from the PII entities described in the list below. \n",
    "        For example, a given name of a person should be enclosed within <FIRST_NAME></FIRST_NAME> tags.\n",
    "        Ensure that all entities are identified. Do not perform false identifications.\n",
    "        \\n\\nList Of Entities\\n{json.dumps(description_dict,indent=2)}\n",
    "        \\n\\n\n",
    "        Are the instructions clear to you?\"\"\"\n",
    "    \n",
    "    asst_msg1 = \"\"\"Yes, the instructions are clear. I will identify and enclose within the corresponding XML tags, \n",
    "        all instances of the specified PII entity types in the user utterance. For example, \n",
    "        <FIRST_NAME><Given name of a person></FIRST_NAME>, <PASSPORT_NUMBER><Number identifying a person’s passport></PASSPORT_NUMBER>, etc. \n",
    "        leaving the rest of the user utterance unchanged.\"\"\"\n",
    "    \n",
    "    usr_msg2 = \"My name is John Doe, and I can be contacted at meet_me_john@gmail.com\"\n",
    "    \n",
    "    asst_msg2 = \"My name is <FIRST_NAME>John</FIRST_NAME> <LAST_NAME>Doe</LAST_NAME>, and I can be contacted at <EMAIL>meet_me_john@gmail.com</EMAIL>\"\n",
    "    \n",
    "    usr_msg3 = \"Give a brief explanation of why your answer is correct.\"\n",
    "\n",
    "\n",
    "    asst_msg3 = \"\"\"My answer is correct because I identified the specified PII entity types in the user utterance and enclosed them within the corresponding XML tags.\n",
    "\n",
    "                    - \"John\" is a given name, so it was enclosed within <FIRST_NAME> tags.\n",
    "                    - \"Doe\" is a surname, so it was enclosed within <LAST_NAME> tags.\n",
    "                    - \"meet_me_john@gmail.com\" is an email address, so it was enclosed within <EMAIL> tags.\"\"\"\n",
    "    \n",
    "    usr_msg4 = \"\"\"Great! I am now going to give you another user utterance. Please detect PII entities in it according to the previous instructions. Do not include an explanation in your answer.\"\"\"\n",
    "    \n",
    "    asst_msg4 = \"Sure! Please give me the user utterance.\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": usr_msg1},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg1},\n",
    "        {\"role\": \"user\", \"content\": usr_msg2},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg2},\n",
    "        {\"role\": \"user\", \"content\": usr_msg3},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg3},\n",
    "        {\"role\": \"user\", \"content\": usr_msg4},\n",
    "        {\"role\": \"assistant\", \"content\": asst_msg4},\n",
    "        {\"role\": \"user\", \"content\": input_str},\n",
    "    ]\n",
    "    \n",
    "    encoded_input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    return encoded_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T11:05:35.261993Z",
     "iopub.status.busy": "2025-04-12T11:05:35.261707Z",
     "iopub.status.idle": "2025-04-12T11:08:18.512744Z",
     "shell.execute_reply": "2025-04-12T11:08:18.511958Z",
     "shell.execute_reply.started": "2025-04-12T11:05:35.261972Z"
    }
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    data_prompt.format(\n",
    "        #instructions\n",
    "        text,\n",
    "        #answer\n",
    "        \"\",\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ef8a9709bc4022b541f402127304ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af6a1ace99141c9ac03709793232b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99321ff82f74533a0ae5ea4a6b3f179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8c5d32bf564715bbf615ec5c3dd380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2357dd7962401abaa8cc0ea4cae4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585fefbbd78d4b26ab2dae422f294267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/22.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Model can be used from the cloud by this method\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"Arpx22/llama-3.2-1B-Finetuned-ner-finance\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,  # Set to False if you prefer full precision\n",
    "    dtype=None  ,        # Set to torch.float16 if needed\n",
    "    gpu_memory_utilization=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.2\",  # or \"chatml\", \"alpaca\", etc. based on how you finetuned\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    "    map_eos_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer of the question is: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are given a user utterance that may contain Personal Identifiable Information (PII). \n",
      "        You are also given a list of entity types representing Personal Identifiable Information (PII). \n",
      "        Your task is to detect and identify all instances of the supplied PII entity types in the user utterance. \n",
      "        The output must have the same content as the input. Only the tokens that match the PII entities in the \n",
      "        list should be enclosed within XML tags. The XML tag comes from the PII entities described in the list below. \n",
      "        For example, a given name of a person should be enclosed within <FIRST_NAME></FIRST_NAME> tags.\n",
      "        Ensure that all entities are identified. Do not perform false identifications.\n",
      "        \n",
      "\n",
      "List Of Entities\n",
      "{\n",
      "  \"ACCOUNT_PIN\": \"Personal Identification Number for accounts\",\n",
      "  \"API_KEY\": \"Authentication token for accessing APIs\",\n",
      "  \"BANK_ROUTING_NUMBER\": \"Bank\\u2019s routing number used in financial transactions\",\n",
      "  \"BBAN\": \"Basic Bank Account Number used in international banking\",\n",
      "  \"COMPANY\": \"Name of a business or organization\",\n",
      "  \"CREDIT_CARD_NUMBER\": \"Number on a credit card used for payments\",\n",
      "  \"CREDIT_CARD_SECURITY_CODE\": \"CVV or security code on a credit card\",\n",
      "  \"CUSTOMER_ID\": \"Unique identifier assigned to a customer\",\n",
      "  \"DATE\": \"Calendar date (e.g. 2025-04-10)\",\n",
      "  \"DATE_OF_BIRTH\": \"Person\\u2019s date of birth\",\n",
      "  \"DRIVER_LICENSE_NUMBER\": \"License number assigned to a driver\",\n",
      "  \"EMAIL\": \"Email address\",\n",
      "  \"EMPLOYEE_ID\": \"Identifier assigned to an employee\",\n",
      "  \"FIRST_NAME\": \"Given name of a person\",\n",
      "  \"IBAN\": \"International Bank Account Number used in global banking\",\n",
      "  \"IPV4\": \"IPv4 format IP address (e.g. 192.168.1.1)\",\n",
      "  \"IPV6\": \"IPv6 format IP address\",\n",
      "  \"LAST_NAME\": \"Surname or family name of a person\",\n",
      "  \"LOCAL_LATLNG\": \"Geolocation data in latitude and longitude format\",\n",
      "  \"NAME\": \"Full name of a person\",\n",
      "  \"PASSPORT_NUMBER\": \"Number identifying a person\\u2019s passport\",\n",
      "  \"PASSWORD\": \"Confidential login credential\",\n",
      "  \"SSN\": \"Social Security Number (often US)\",\n",
      "  \"STREET_ADDRESS\": \"Street-level residential or business address\",\n",
      "  \"SWIFT_BIC_CODE\": \"Bank Identifier Code used for international wires\",\n",
      "  \"TIME\": \"Time of day (e.g. 14:00, 2 PM)\",\n",
      "  \"USER_NAME\": \"Username used for login or identification\"\n",
      "}\n",
      "        \n",
      "\n",
      "\n",
      "        Are the instructions clear to you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, the instructions are clear. I will identify and enclose within the corresponding XML tags, \n",
      "        all instances of the specified PII entity types in the user utterance. For example, \n",
      "        <FIRST_NAME><Given name of a person></FIRST_NAME>, <PASSPORT_NUMBER><Number identifying a person’s passport></PASSPORT_NUMBER>, etc. \n",
      "        leaving the rest of the user utterance unchanged.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "My name is John Doe, and I can be contacted at meet_me_john@gmail.com<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "My name is <FIRST_NAME>John</FIRST_NAME> <LAST_NAME>Doe</LAST_NAME>, and I can be contacted at <EMAIL>meet_me_john@gmail.com</EMAIL><|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Give a brief explanation of why your answer is correct.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "My answer is correct because I identified the specified PII entity types in the user utterance and enclosed them within the corresponding XML tags.\n",
      "\n",
      "                    - \"John\" is a given name, so it was enclosed within <FIRST_NAME> tags.\n",
      "                    - \"Doe\" is a surname, so it was enclosed within <LAST_NAME> tags.\n",
      "                    - \"meet_me_john@gmail.com\" is an email address, so it was enclosed within <EMAIL> tags.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Great! I am now going to give you another user utterance. Please detect PII entities in it according to the previous instructions. Do not include an explanation in your answer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Sure! Please give me the user utterance.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Indian Overseas Bank reduces Repo Linked Lending Rate by 25 basis points\n",
      "Indian Overseas Bank has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by 2030, exports to triple: NITI Aayog\n",
      "NITI Aayog envisions India's automotive component production soaring to USD 145 billion by 2030, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by Union Bank of India are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in May\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from May 1-4, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "Radisson Hotel Group is bullish on India's hotel development potential, aiming to reach 500 hotels by the end of the decade by signing 50 <|eot_id|><|reserved_special_token_17|>assistantespoň\n",
      "\n",
      "<COMPANY>Indian Overseas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY>Indian Overseas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY>NITI Aayog</COMPANY>\n",
      "<COMPANY>NITI Aayog</COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_117|><|reserved_special_token_217|>assistant<|reserved_special_token_78|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_182|><|reserved_special_token_99|>assistant<|reserved_special_token_146|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_225|><|reserved_special_token_168|>assistant<|reserved_special_token_68|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_37|><|reserved_special_token_161|>assistant<|reserved_special_token_19|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_229|><|reserved_special_token_176|>assistant<|reserved_special_token_41|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_57|><|reserved_special_token_100|>assistant<|reserved_special_token_165|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_61|><|reserved_special_token_131|>assistant<|reserved_special_token_110|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_91|><|reserved_special_token_175|>assistant<|reserved_special_token_35|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_39|><|reserved_special_token_2|>assistant<|reserved_special_token_122|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_97|><|reserved_special_token_230|>assistant<|reserved_special_token_42|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_0|><|reserved_special_token_136|>assistant<|reserved_special_token_201|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_129|>ıldığındaassistant<|reserved_special_token_68|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 кадем<|reserved_special_token_160|>assistant<|reserved_special_token_78|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_121|><|reserved_special_token_212|>assistant<|reserved_special_token_120|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_194|><|reserved_special_token_187|>assistant<|reserved_special_token_235|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_214|><|reserved_special_token_131|>assistant<|reserved_special_token_64|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_13|><|reserved_special_token_191|>assistant<|reserved_special_token_113|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_52|> располагassistant<|python_tag|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_199|><|reserved_special_token_25|>assistant<|reserved_special_token_176|>\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government....\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 <|reserved_special_token_228|><|reserved_special_token_231|>assistantЎыџN\n",
      "\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "<COMPANY><COMPANY>Indian Over</COMPANY>seas Bank</COMPANY> has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY>\n",
      "<COMPANY><COMPANY>NITI Aayog</COMPANY></COMPANY> envisions India's automotive component production soaring to USD 145 billion by <DATE>2030</DATE>, with exports tripling to USD 60 billion. A...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(testing_prompt_xml(entity_description_dict,test_text,tokenizer), max_new_tokens = 8192, use_cache = True)\n",
    "answer=tokenizer.batch_decode(outputs)\n",
    "print(\"Answer of the question is:\", answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "# Helper Function for viewing the extracted entities\n",
    "def extract_outermost_entities(text: str, valid_entity_types: List[str]) -> List[Tuple[str, str]]:\n",
    "    entities = []\n",
    "    \n",
    "    for entity in valid_entity_types:\n",
    "        # Match the outermost <ENTITY> ... </ENTITY> including malformed variants\n",
    "        pattern = fr\"\"\"\n",
    "            (<+{entity}>+)            # Opening tag (malformed tolerated)\n",
    "            (.*?)                     # Non-greedy capture of content\n",
    "            (</+{entity}>+)           # Closing tag (malformed tolerated)\n",
    "        \"\"\"\n",
    "        # Use re.DOTALL to match across lines and VERBOSE for formatting\n",
    "        matches = list(re.finditer(pattern, text, flags=re.DOTALL | re.VERBOSE))\n",
    "        \n",
    "        # To avoid nested matches, we skip overlaps\n",
    "        used_spans = set()\n",
    "        for match in matches:\n",
    "            span = match.span()\n",
    "            if any(start <= span[0] < end or start < span[1] <= end for start, end in used_spans):\n",
    "                continue  # Skip if overlaps with already accepted match\n",
    "            \n",
    "            inner_text = match.group(2).strip()\n",
    "\n",
    "            # Remove nested tags inside entity text\n",
    "            inner_text = re.sub(r\"<[^>]+>\", \"\", inner_text).strip()\n",
    "            cleaned_text = re.sub(r'{}'.format(entity), '', inner_text)\n",
    "            cleaned_text = re.sub(r'[<>/\\\\\\n\\t]', '', cleaned_text)\n",
    "            cleaned_text = cleaned_text.strip()\n",
    "        \n",
    "            # Deduplicate repeating sequences of words using regex\n",
    "            pattern = r'\\b(\\w+(?:\\s+\\w+)*)\\b(?:\\s+\\1\\b)+'\n",
    "            deduplicated_text = re.sub(pattern, r'\\1', cleaned_text)\n",
    "            \n",
    "            entities.append((entity, deduplicated_text))\n",
    "            used_spans.add(span)\n",
    "    entities = list(set(entities))\n",
    "    existing_ent = [('PASSPORT_NUMBER', ''),('LAST_NAME', 'Doe'),('FIRST_NAME', 'John'),('EMAIL', 'meet_me_john@gmail.com'),('FIRST_NAME', '')]\n",
    "    entities = [ent for ent in entities if ent not in existing_ent]\n",
    "    return list(set(entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_found = extract_outermost_entities(answer[0],list(entity_description_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('COMPANY', 'Radisson Hotel Group')\n",
      "('COMPANY', 'Indian Over')\n",
      "('DATE', 'May')\n",
      "('DATE', 'May 1-4')\n",
      "('DATE', 'the end of the decade')\n",
      "('COMPANY', 'Union Bank of India')\n",
      "('COMPANY', 'Indian Overseas Bank')\n",
      "('DATE', '2030')\n",
      "('COMPANY', 'NITI Aayog')\n"
     ]
    }
   ],
   "source": [
    "for ent in entities_found:\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model was trained only on two complete iteration and the generation take a lot of time (4 to 5 minutes for 1000 tokens) as my GPU is not that good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "def tag_entities_in_text(text: str, entities: List[Tuple[str, str]]) -> str:\n",
    "    tagged_text = text\n",
    "    used_spans = []\n",
    "\n",
    "    # Sort entities to tag longer ones first to avoid substring overlaps\n",
    "    for entity_type, entity_value in sorted(entities, key=lambda x: -len(x[1])):\n",
    "        # Escape regex special characters in entity value\n",
    "        escaped_value = re.escape(entity_value)\n",
    "\n",
    "        # Build pattern to search the first unmatched occurrence\n",
    "        pattern = re.compile(escaped_value)\n",
    "\n",
    "        def replacement_fn(match):\n",
    "            # Check if this span is already used\n",
    "            span = match.span()\n",
    "            if any(start < span[1] and span[0] < end for start, end in used_spans):\n",
    "                return match.group(0)  # Don't tag again\n",
    "\n",
    "            used_spans.append(span)\n",
    "            return f\"<{entity_type}>{match.group(0)}</{entity_type}>\"\n",
    "\n",
    "        # Only replace the first occurrence not already tagged\n",
    "        tagged_text = pattern.sub(replacement_fn, tagged_text, count=1)\n",
    "\n",
    "    return tagged_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<COMPANY>Indian Overseas Bank</COMPANY> reduces Repo Linked Lending Rate by 25 basis points\n",
      "Indian Overseas Bank has lowered its Repo Linked Lending Rate by 25 basis points, effective immediately. This decision follows the Reserve B...\n",
      "\n",
      "Delhi: Stampede-like situation reported at Indira Gandhi International Airport as 50 flights get delayed due to dust storm\n",
      "A severe dust storm paralyzed Delhi and the NCR, causing widespread flight disruptions at Indira Gandhi International Airport. Over 50 domes...\n",
      "\n",
      "India's auto component industry to hit USD 145 billion by <DATE>2030</DATE>, exports to triple: <COMPANY>NITI Aayog</COMPANY>\n",
      "NITI Aayog envisions India's automotive component production soaring to USD 145 billion by 2030, with exports tripling to USD 60 billion. A ...\n",
      "\n",
      "Banks scouting buyers for Sahara Star Hotel loans\n",
      "Lenders led by <COMPANY>Union Bank of India</COMPANY> are set to auction the Rs 700 crore debt owed by Sahara Star Hotel after the insolvency plea was rejected...\n",
      "\n",
      "Global media titans to converge in Mumbai for first-ever WAVES summit in <DATE>May</DATE>\n",
      "Mumbai is gearing up to host the inaugural World Audio Visual & Entertainment Summit (WAVES) from <DATE>May 1-4</DATE>, backed by the Indian government. ...\n",
      "\n",
      "India is the place to be: Radisson Top Exec\n",
      "<COMPANY>Radisson Hotel Group</COMPANY> is bullish on India's hotel development potential, aiming to reach 500 hotels by <DATE>the end of the decade</DATE> by signing 50 \n"
     ]
    }
   ],
   "source": [
    "print(tag_entities_in_text(test_text,entities_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('COMPANY', 'Radisson Hotel Group'),\n",
       " ('COMPANY', 'Indian Over'),\n",
       " ('DATE', 'May'),\n",
       " ('DATE', 'May 1-4'),\n",
       " ('DATE', 'the end of the decade'),\n",
       " ('COMPANY', 'Union Bank of India'),\n",
       " ('COMPANY', 'Indian Overseas Bank'),\n",
       " ('DATE', '2030'),\n",
       " ('COMPANY', 'NITI Aayog')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4565373,
     "sourceId": 7806065,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7111586,
     "sourceId": 11362311,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
